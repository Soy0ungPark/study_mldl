{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soy0ungPark/study_mldl/blob/master/18_reinforcement_learning_test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning 에이전트 구현"
      ],
      "metadata": {
        "id": "8hks5cI4rk4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class QLearning:\n",
        "  def __init__(self, actions):\n",
        "    self.ACTIONS = np.array(actions)\n",
        "    self.step_size = 0.01\n",
        "    self.discount_factor = 0.9\n",
        "    self.epsilon = 0.1\n",
        "    self.q_table = defaultdict(lambda:[0.0 for _ in range(len(actions))])\n",
        "\n",
        "  def get_action(self, state):      # 행동 가져오기\n",
        "    if np.random.rand() < self.epsilon:\n",
        "        action = np.random.choice(self.ACTIONS)\n",
        "    else:\n",
        "        q_list = self.q_table[str(state)]\n",
        "        action = self.ACTIONS[np.random.choice(np.argwhere(q_list == np.amax(q_list)).flatten().tolist())]\n",
        "    return action\n",
        "\n",
        "  def save(self, root):\n",
        "    df = pd.DataFrame([[state, self.q_table[state]] for state in self.q_table.keys()], columns=['states', 'q_list'])\n",
        "    df.to_csv(root)\n",
        "\n",
        "  def load(self, root):\n",
        "    def str_to_list(s):\n",
        "      s = s.split('[')\n",
        "      s = s[1].split(']')\n",
        "      s = s[0].split(', ')\n",
        "      for i in range(len(s)):\n",
        "        s[i] = float(s[i])\n",
        "      return s\n",
        "\n",
        "    self.q_table = defaultdict(lambda:[0.0 for _ in range(len(self.Actions))])\n",
        "    df = pd.read_csv(root)\n",
        "    for idx in df.index:\n",
        "      self.q_table[df['states'][idx]] = str_to_list(df['q_list'][idx])\n",
        "\n",
        "  def learn(self, state, action, reward, next_state):\n",
        "\n",
        "    state, next_state = str(state), str(next_state)\n",
        "\n",
        "    current_q = self.q_table[state][np.argwhere(self.ACTIONS==action)[0][0]]\n",
        "    next_state_q =  max(self.q_table[next_state])  # 큐러닝의 큐함수 업데이트 식: S A R S' 샘플을 받아 상태 S'에서의 최대 큐함수를 구하고 업데이트\n",
        "\n",
        "    td = reward + self.discount_factor * next_state_q - current_q\n",
        "    new_q = current_q + self.step_size * td\n",
        "    self.q_table[state][np.argwhere(self.ACTIONS==action)[0][0]] = new_q"
      ],
      "metadata": {
        "id": "M2ojp_OGrj-Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN 구현\n"
      ],
      "metadata": {
        "id": "v6TbCJx1qCu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aDRPlhdpp-MR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomUniform\n",
        "import tensorflow as tf\n",
        "\n",
        "class NN(tf.keras.Model):\n",
        "  def __init__(self, action_size):\n",
        "    super(NN, self).__init__()\n",
        "    self.fc1 = Dense(24, activation='relu')\n",
        "    self.fc2 = Dense(24, activation='relu')\n",
        "    self.fc_out = Dense(action_size, kernel_initializer=RandomUniform(-1e-3, 1e-3))\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    q = self.fc_out(x)\n",
        "    return q\n",
        "\n",
        "class DQN:\n",
        "  def __init__(self, state_size, aciton_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = aciton_size\n",
        "\n",
        "    self.discount_factor = 0.99\n",
        "    self.learning_rate = 0.001\n",
        "    self.epsilon = 1.0\n",
        "    self.epsilon_decay = 0.999\n",
        "    self.epsilon_min = 0.001\n",
        "    self.batch_size = 64\n",
        "    self.train_start = 1000\n",
        "\n",
        "    self.memory = deque(maxlen=2000)  # queue(큐) 구조를 사용해 FIFO 선입선출\n",
        "\n",
        "    self.model = DQN(self.action_size)\n",
        "    self.target_model = DQN(self.action_size)\n",
        "    self.optimizer = Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "    self.update_target_model()\n",
        "\n",
        "  def update_target_model(self):\n",
        "    self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "  def get_action(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return random.randrange(self.action_size)\n",
        "    else:\n",
        "      q = self.model(state) #리스트 형태로 반환됨\n",
        "      return np.argmax(q[0])\n",
        "\n",
        "  def append_sample(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))  # DQN -> S, A, R, S' 원소 이용\n",
        "\n",
        "  def train_model(self):\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    mini_batch = random.sample(self.memory, self.batch_size)\n",
        "    states = np.array([sample[0][0] for sample in mini_batch])\n",
        "    actions = np.array([sample[1] for sample in mini_batch])\n",
        "    rewards = np.array([sample[2] for sample in mini_batch])\n",
        "    next_states = np.array([sample[3][0] for sample in mini_batch])\n",
        "    dones = np.array([sample[4] for sample in mini_batch])\n",
        "\n",
        "    model_params = self.model.trainable_variables\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicts = self.model(states)\n",
        "      one_hot_action = tf.one_hot(actions , self.action_size)\n",
        "      predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
        "\n",
        "      target_predicts = self.target_model(next_states)\n",
        "      target_predicts = tf.stop_gradient(target_predicts)\n",
        "\n",
        "      max_q = np.amax(target_predicts, axis=-1)\n",
        "      targets = rewards + (1-dones) * self.discount_factor * max_q\n",
        "      loss = tf.reduce_mean(tf.square(targets - predicts))\n",
        "\n",
        "    grads = tape.gradient(loss, model_params)\n",
        "    self.optimizer.apply_gradients(zip(grads, model_params))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97GawBVVFO9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}